# -*- coding: utf-8 -*-
"""ML_classification_pipeline_complete.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OhE8M3Fofq5iBdSH53YVIMhPLvoFrhlD
"""

from sklearn.svm import SVC
from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from google.colab import files
from sklearn.model_selection import StratifiedKFold
import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np

"""Google colab"""

from google.colab import drive
drive.mount('/content/drive')

"""Load CSV (DATASET)"""

# Step 1: Upload and load CSV
csv_path = '/content/drive/MyDrive/Progetto/dataset_ridotto_new_sellion.csv'
df = pd.read_csv(csv_path)

# test set unknown
test_set_unknown_path = '/content/drive/MyDrive/Progetto/test_unknown_new.csv'
test_set_unknown = pd.read_csv(test_set_unknown_path)

name_counts = df["name"].value_counts()
valid_names = name_counts[name_counts >= 5].index
df_filtered = df[df["name"].isin(valid_names)].copy()

df_filtered

"""Division CS and TS"""

def split_dataframe(df, seed=None):
    if seed is not None:
        np.random.seed(seed)

    constr_rows = []
    test_rows = []

    for subject_id, group in df.groupby(df.columns[0]):
        group = group.copy()
        happy_row = group[group["emotion"] == "HAPPY"]
        if happy_row.empty:
            raise ValueError(f"No HAPPY pose found for subject {subject_id}")
        other_rows = group[group["emotion"] != "HAPPY"]
        test_row = other_rows.sample(n=1, random_state=seed)
        constr_group = pd.concat([happy_row, other_rows.drop(test_row.index)])
        constr_rows.append(constr_group)
        test_rows.append(test_row)

    df_constr = pd.concat(constr_rows).reset_index(drop=True)
    df_test = pd.concat(test_rows).reset_index(drop=True)
    return df_constr, df_test

df_construction, df_test = split_dataframe(df_filtered, seed=42)

df_construction

df_test

"""IMPUTATION MISSING VALUES"""

def impute_by_name(df, name_column='name'):
    df_imputed = df.copy()

    for name_value, group in df.groupby(name_column):
        for col in df.columns:
            if col == name_column:
                continue
            if group[col].isna().any():
                value_to_fill = group[col].mean()
                idxs_to_fill = group[group[col].isna()].index
                df_imputed.loc[idxs_to_fill, col] = value_to_fill

    return df_imputed

df_construction_imputed = impute_by_name(df_construction, name_column='name')

df_construction_imputed

# The NaN values in the test set are handled using information from the construction set.
column_means = df_construction_imputed.mean(numeric_only=True)
df_test_imputed = df_test.fillna(column_means)
df_test_unknown_imputed = test_set_unknown.fillna(column_means)

df_test_imputed

df_test_unknown_imputed

"""## TUNING"""

def split_construction_set(df_constr, seed=None):
    if seed is not None:
        np.random.seed(seed)

    train_rows = []
    val_rows = []

    for subject_id, group in df_constr.groupby(df_constr.columns[0]):
        group = group.copy()
        # Select one row for each of the three distinct emotions
        train_emotions = []
        for emotion in ['NEUTRAL', 'HAPPY', 'SMILE']:
            emotion_row = group[group["emotion"] == emotion]
            if not emotion_row.empty:
                # Take the first occurrence of each emotion
                train_emotions.append(emotion_row.iloc[[0]])
        # Concatenate the three rows for training
        train_rows.append(pd.concat(train_emotions))

        # The remaining row (the repeated NEUTRAL or SMILE) goes to validation
        used_indices = pd.concat(train_emotions).index
        val_row = group.drop(index=used_indices)
        if not val_row.empty:
            val_rows.append(val_row)

    df_train = pd.concat(train_rows).reset_index(drop=True)
    df_val = pd.concat(val_rows).reset_index(drop=True)

    return df_train, df_val

# train set e val set
train_set, val_set  = split_construction_set(df_construction_imputed)

# test set
test_set = df_test_imputed

"""Split the dataset into distances and geometric descriptors"""

#Dataset Euclidean distance
df_base = train_set.iloc[:, :2]
df_class = train_set.iloc[:, -1]
df_dist = train_set.loc[:, train_set.columns.str.contains('dist')]

df_euclidean_train = pd.concat([df_base, df_dist, df_class ], axis=1)

#Dataset geometric descriptors

used_columns  = df_dist.columns.tolist()
remaining_columns = [col for col in train_set.columns if col not in used_columns]

df_geometric_train = train_set[remaining_columns]

df_euclidean_train

df_geometric_train

"""Ratio for feature selection"""

def compute_discriminative_ratio(df, normalize=True, title="Feature Ratios"):
    feature_columns = df.columns[2:-1]
    class_column = df.columns[-1]

    df_processed = df.copy()

    if normalize:
        scaler = StandardScaler()
        df_processed[feature_columns] = scaler.fit_transform(df_processed[feature_columns])

    # Compute between-class and within-class variance for each feature
    ratios = {}

    for feature in feature_columns:
        global_mean = df_processed[feature].mean()
        between_class_var = 0.0
        within_class_var = 0.0

        for class_label, group in df_processed.groupby(class_column):

            n_i = len(group)
            mean_i = group[feature].mean()

            between_class_var += (mean_i - global_mean)**2
            within_class_var += (1/n_i)* ((group[feature] - mean_i)**2).sum()

        ratio = between_class_var / within_class_var if within_class_var != 0 else np.inf
        ratios[feature] = ratio

    ratios_df = pd.DataFrame.from_dict(ratios, orient='index', columns=['Discriminative Ratio'])
    ratios_df = ratios_df.sort_values(by='Discriminative Ratio', ascending=False)

    plt.figure(figsize=(14, 6))
    plt.bar(ratios_df.index, ratios_df['Discriminative Ratio'])
    plt.xticks(rotation=90)
    plt.title(f"Discriminative Ratio per Feature - {title}")
    plt.ylabel("Between / Within-class Ratio")
    plt.tight_layout()
    plt.show()

    return ratios_df

ratios_euclidean = compute_discriminative_ratio(df_euclidean_train, normalize=False, title="Euclidean Features")

ratios_geom = compute_discriminative_ratio(df_geometric_train, normalize=False, title="Geometric Features")

def create_filtered_set_from2(df_euclidean_set, df_geometric_set, distance_selected, geometric_selected):
    df_info = df_euclidean_set.iloc[:, :2]
    df_label = df_euclidean_set.iloc[:, -1]
    df_features = pd.concat([
        df_euclidean_set[distance_selected],
        df_geometric_set[geometric_selected]
    ], axis=1)
    df_set_filtered = pd.concat([df_info, df_features, df_label], axis=1)
    return df_set_filtered

def create_filtered_set_from1(df_euclidean_geometric_set, distance_selected, geometric_selected):
    df_info = df_euclidean_geometric_set.iloc[:, :2]
    df_label = df_euclidean_geometric_set.iloc[:, -1]
    df_features = df_euclidean_geometric_set[distance_selected + geometric_selected]
    df_set_filtered = pd.concat([df_info, df_features, df_label], axis=1)
    return df_set_filtered

def plot_max_prob_distribution(clf, X_val, X_unknown, clf_name):
    probs_val = clf.predict_proba(X_val)
    max_probs_val = probs_val.max(axis=1)

    plt.figure(figsize=(8, 4))
    plt.hist(max_probs_val, bins=30, alpha=0.7, label='Validation set')
    plt.xlabel('Maximum probability')
    plt.ylabel('Frequency')
    plt.title(f'Distribution of maximum probabilities - {clf_name}')
    plt.legend()
    plt.tight_layout()
    plt.show()

import numpy as np
import pandas as pd
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

# Parametri
threshold = 0.09
n_euclidean = 12
n_geometric = 18

distance_selected = list(ratios_euclidean.index[0:n_euclidean])
geometric_selected = list(ratios_geom.index[0:n_geometric])

# Assicurati che tutti i set abbiano le colonne geometriche necessarie
for df in [val_set, test_set, df_test_unknown_imputed]:
    for col in geometric_selected:
        if col not in df.columns:
            df[col] = np.random.rand(len(df))

# Creazione set filtrati
df_train_set_filtered  = create_filtered_set_from2(df_euclidean_train, df_geometric_train, distance_selected, geometric_selected)
df_val_set_filtered  = create_filtered_set_from1(val_set, distance_selected, geometric_selected)
df_test_known_set_filtered  = create_filtered_set_from1(test_set, distance_selected, geometric_selected)
df_test_unknown_set_filtered = create_filtered_set_from1(df_test_unknown_imputed, distance_selected, geometric_selected)

df_test_set_filtered = pd.concat([df_test_known_set_filtered, df_test_unknown_set_filtered], axis=0, ignore_index=True)

# Features e classi
X_train = df_train_set_filtered.iloc[:, 2:-1].values.astype(float)
y_train = df_train_set_filtered.iloc[:, -1].values.astype(str)
X_val = df_val_set_filtered.iloc[:, 2:-1].values.astype(float)
y_val = df_val_set_filtered.iloc[:, -1].values.astype(str)
X_test = df_test_set_filtered.iloc[:, 2:-1].values.astype(float)
y_test = df_test_set_filtered.iloc[:, -1].values.astype(str)

X_test_unknown = df_test_unknown_set_filtered.iloc[:, 2:-1].values.astype(float)
y_test_unknown = df_test_unknown_set_filtered.iloc[:, -1].values.astype(str)

# Normalizzazione
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

X_test_unknown_scaled = scaler.transform(X_test_unknown)

# Funzione per predizione con soglia e classe 'altro'
def predict_with_threshold(clf, X, threshold=0.09):
    if hasattr(clf, "predict_proba"):
        probs = clf.predict_proba(X)
        max_probs = probs.max(axis=1)
        pred_indices = probs.argmax(axis=1)
        pred_labels = clf.classes_[pred_indices].copy()
        pred_labels[max_probs < threshold] = 0
    else:
        pred_labels = clf.predict(X)
    return pred_labels


# Classificatore SVM
th_svm = 0.09
svm_best = SVC(C=100, kernel='rbf', probability=True, random_state=42)
svm_best.fit(X_train_scaled, y_train)
train_pred = predict_with_threshold(svm_best, X_train_scaled, th_svm)
train_bal_acc = accuracy_score(y_train, train_pred)
val_pred = predict_with_threshold(svm_best, X_val_scaled, th_svm)
val_bal_acc = accuracy_score(y_val, val_pred)
test_pred = predict_with_threshold(svm_best, X_test_scaled, th_svm)
test_bal_acc = accuracy_score(y_test, test_pred)

plot_max_prob_distribution(svm_best, X_val_scaled, X_test_unknown_scaled, "SVM")


# Classificatore kNN
th_knn = 0.8
knn_best = KNeighborsClassifier(n_neighbors=2)
knn_best.fit(X_train_scaled, y_train)
train_pred_knn = predict_with_threshold(knn_best, X_train_scaled, th_knn)
bal_acc_train_knn = accuracy_score(y_train, train_pred_knn)
val_pred_knn = predict_with_threshold(knn_best, X_val_scaled, th_knn)
bal_acc_val_knn = accuracy_score(y_val, val_pred_knn)
test_pred_knn = predict_with_threshold(knn_best, X_test_scaled, th_knn)
bal_acc_test_knn = accuracy_score(y_test, test_pred_knn)

plot_max_prob_distribution(knn_best, X_val_scaled, X_test_unknown_scaled, "KNN")


# Classificatore RandomForest
th_rf = 0.09
rf_best = RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=2, min_samples_leaf=1, random_state=42)
rf_best.fit(X_train_scaled, y_train)
y_pred_train_rf = predict_with_threshold(rf_best, X_train_scaled, th_rf)
bal_acc_train_rf = accuracy_score(y_train, y_pred_train_rf)
y_pred_val_rf = predict_with_threshold(rf_best, X_val_scaled, th_rf)
bal_acc_val_rf = accuracy_score(y_val, y_pred_val_rf)
y_pred_test_rf = predict_with_threshold(rf_best, X_test_scaled, th_rf)
bal_acc_test_rf = accuracy_score(y_test, y_pred_test_rf)

plot_max_prob_distribution(rf_best, X_val_scaled, X_test_unknown_scaled, "Random Forest")


# Classificatore Neural Network (MLP)
th_mlp = 0.7
nn_model = MLPClassifier(hidden_layer_sizes=(100), max_iter=1000, random_state=42)
nn_model.fit(X_train_scaled, y_train)
y_pred_train_nn = predict_with_threshold(nn_model, X_train_scaled, th_mlp)
bal_acc_train_nn = accuracy_score(y_train, y_pred_train_nn)
y_pred_val_nn = predict_with_threshold(nn_model, X_val_scaled, th_mlp)
bal_acc_val_nn = accuracy_score(y_val, y_pred_val_nn)
y_pred_test_nn = predict_with_threshold(nn_model, X_test_scaled, th_mlp)
bal_acc_test_nn = accuracy_score(y_test, y_pred_test_nn)

plot_max_prob_distribution(nn_model, X_val_scaled, X_test_unknown_scaled, "MLP")


# Salvataggio risultati
results_complete = []
results_complete.append({
    "n_euclidean": n_euclidean,
    "n_geometric": n_geometric,
    "SVM_train": train_bal_acc,
    "SVM_val": val_bal_acc,
    "SVM_test": test_bal_acc,
    "KNN_train": bal_acc_train_knn,
    "KNN_val": bal_acc_val_knn,
    "KNN_test": bal_acc_test_knn,
    "RF_train": bal_acc_train_rf,
    "RF_val": bal_acc_val_rf,
    "RF_test": bal_acc_test_rf,
    "MLP_train": bal_acc_train_nn,
    "MLP_val": bal_acc_val_nn,
    "MLP_test": bal_acc_test_nn
})

df_results_complete = pd.DataFrame(results_complete)
print(df_results_complete)

# Lista dei classificatori e colonne corrispondenti
classificatori = ['SVM', 'KNN', 'RF', 'MLP']
colonne = ['train', 'val', 'test']

# Costruisci la tabella
table_data = []
for clf in classificatori:
    row = [
        df_results_complete[f"{clf}_train"].iloc[0],
        df_results_complete[f"{clf}_val"].iloc[0],
        df_results_complete[f"{clf}_test"].iloc[0]
    ]
    table_data.append(row)

df_tabella = pd.DataFrame(table_data, columns=colonne, index=classificatori)
print(df_tabella)

x = np.arange(len(df_tabella.index))
width = 0.25

colors = ['#4F81BD', '#6CA6CD', '#B0C4DE']

fig, ax = plt.subplots(figsize=(10, 6))

bars_train = ax.bar(x - width, df_tabella['train'], width, label='Train', color=colors[0])
bars_val = ax.bar(x, df_tabella['val'], width, label='Validation', color=colors[1])
bars_test = ax.bar(x + width, df_tabella['test'], width, label='Test', color=colors[2])

for bars in [bars_train, bars_val, bars_test]:
    for bar in bars:
        yval = bar.get_height()
        ax.text(
            bar.get_x() + bar.get_width() / 2,
            yval + 0.01,
            f'{yval:.2f}',
            ha='center',
            va='bottom',
            fontsize=10
        )

ax.set_xlabel('Classifier')
ax.set_ylabel('Recognition Rate')
ax.set_title('Recognition Rate (Train, Validation, Test)')
ax.set_xticks(x)
ax.set_xticklabels(df_tabella.index)
ax.legend(loc='upper left', bbox_to_anchor=(1, 1))
plt.tight_layout()
plt.show()

def plot_confusion_matrix(y_true, y_pred, labels):
    cm = confusion_matrix(y_true, y_pred, labels=labels) #labels=np.unique(y)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
    fig, ax = plt.subplots(figsize=(15, 10))
    disp.plot(ax=ax, cmap=plt.cm.Blues)
    plt.title("Confusion Matrix")
    plt.show()

labels = list(range(1, 34)) + ['0']
plot_confusion_matrix(y_test, test_pred, labels)

# SVM

mask_registered = y_test != '0'
mask_unknown = y_test == '0'

# registered
n_registered = np.sum(mask_registered)
correct_registered = np.sum((y_test == test_pred) & mask_registered)
incorrect_registered = n_registered - correct_registered

# unknown
n_unknown = np.sum(mask_unknown)
correct_unknown = np.sum((y_test == test_pred) & mask_unknown)
incorrect_unknown = n_unknown - correct_unknown

df_summary = pd.DataFrame({
    'No of samples': [n_registered, n_unknown],
    'Correctly classified': [correct_registered, correct_unknown],
    'Incorrectly classified': [incorrect_registered, incorrect_unknown]
}, index=['Registered Patient', 'Unkown Patient'])

print(df_summary)

# KNN

mask_registered = y_test != '0'
mask_unknown = y_test == '0'

# registered
n_registered = np.sum(mask_registered)
correct_registered = np.sum((y_test == test_pred_knn) & mask_registered)
incorrect_registered = n_registered - correct_registered

# unknown
n_unknown = np.sum(mask_unknown)
correct_unknown = np.sum((y_test == test_pred_knn) & mask_unknown)
incorrect_unknown = n_unknown - correct_unknown

df_summary = pd.DataFrame({
    'No of samples': [n_registered, n_unknown],
    'Correctly classified': [correct_registered, correct_unknown],
    'Incorrectly classified': [incorrect_registered, incorrect_unknown]
}, index=['Registered Patient', 'Unkown Patient'])

print(df_summary)

# RF

mask_registered = y_test != '0'
mask_unknown = y_test == '0'

# registered
n_registered = np.sum(mask_registered)
correct_registered = np.sum((y_test == y_pred_test_rf) & mask_registered)
incorrect_registered = n_registered - correct_registered

# unknown
n_unknown = np.sum(mask_unknown)
correct_unknown = np.sum((y_test == y_pred_test_rf) & mask_unknown)
incorrect_unknown = n_unknown - correct_unknown

df_summary = pd.DataFrame({
    'No of samples': [n_registered, n_unknown],
    'Correctly classified': [correct_registered, correct_unknown],
    'Incorrectly classified': [incorrect_registered, incorrect_unknown]
}, index=['Registered Patient', 'Unkown Patient'])

print(df_summary)

# MLP

mask_registered = y_test != '0'
mask_unknown = y_test == '0'

# registered
n_registered = np.sum(mask_registered)
correct_registered = np.sum((y_test == y_pred_test_nn) & mask_registered)
incorrect_registered = n_registered - correct_registered

# unknown
n_unknown = np.sum(mask_unknown)
correct_unknown = np.sum((y_test == y_pred_test_nn) & mask_unknown)
incorrect_unknown = n_unknown - correct_unknown

df_summary = pd.DataFrame({
    'No of samples': [n_registered, n_unknown],
    'Correctly classified': [correct_registered, correct_unknown],
    'Incorrectly classified': [incorrect_registered, incorrect_unknown]
}, index=['Registered Patient', 'Unkown Patient'])

print(df_summary)

"""

---


Tuning per scelta feature

---

"""

# TUNING FEATURES
from tqdm.notebook import tqdm
from sklearn.tree import DecisionTreeClassifier

results = []

for n_euclidean in tqdm(range(1, 21), desc="n_euclidean"):
    for n_geometric in range(1, 21):

        # Selezione delle feature
        distance_selected = list(ratios_euclidean.index[0:n_euclidean])
        geometric_selected = list(ratios_geom.index[0:n_geometric])

        df_train_set_filtered  = create_filtered_set_from2(df_euclidean_train, df_geometric_train, distance_selected, geometric_selected)
        df_val_set_filtered  = create_filtered_set_from1(val_set, distance_selected, geometric_selected)
        df_test_set_filtered  = create_filtered_set_from1(test_set, distance_selected, geometric_selected)



        # Features and class
        X_train = df_train_set_filtered.iloc[:, 2:-1].values.astype(float)  # All features except class
        y_train = df_train_set_filtered.iloc[:, -1].values.astype(str)      # Last column as class
        X_val = df_val_set_filtered.iloc[:, 2:-1].values.astype(float)  # All features except class
        y_val = df_val_set_filtered.iloc[:, -1].values.astype(str)      # Last column as class
        X_test = df_test_set_filtered.iloc[:, 2:-1].values.astype(float)  # All features except class
        y_test = df_test_set_filtered.iloc[:, -1].values.astype(str)      # Last column as class

        # Normalization
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_val_scaled = scaler.transform(X_val)
        X_test_scaled = scaler.transform(X_test)

        # Classificatorie - SVM
        svm_best = SVC(C=100, kernel='rbf', random_state=42)
        svm_best.fit(X_train_scaled, y_train)
        train_pred = svm_best.predict(X_train_scaled)
        train_bal_acc = accuracy_score(y_train, train_pred)
        val_pred = svm_best.predict(X_val_scaled)
        val_bal_acc = accuracy_score(y_val, val_pred)
        test_pred = svm_best.predict(X_test_scaled)
        test_bal_acc = accuracy_score(y_test, test_pred)

        # Classificatore - AdaBoost
        # base_estimator = DecisionTreeClassifier(max_depth=7)
        # adaboost_model = AdaBoostClassifier(
        #     estimator=base_estimator,
        #     n_estimators=500,
        #     random_state=42
        # )
        # adaboost_model.fit(X_train_scaled, y_train)
        # y_pred_train = adaboost_model.predict(X_train_scaled)
        # bal_acc_train = accuracy_score(y_train, y_pred_train)
        # y_pred_val = adaboost_model.predict(X_val_scaled)
        # bal_acc_val = accuracy_score(y_val, y_pred_val)
        # y_pred_test = adaboost_model.predict(X_test_scaled)
        # bal_acc_test = accuracy_score(y_test, y_pred_test)

        # Classificatore - kNN
        knn_best = KNeighborsClassifier(n_neighbors=3)
        knn_best.fit(X_train_scaled, y_train)
        train_pred_knn = knn_best.predict(X_train_scaled)
        bal_acc_train_knn = accuracy_score(y_train, train_pred_knn)
        val_pred_knn = knn_best.predict(X_val_scaled)
        bal_acc_val_knn = accuracy_score(y_val, val_pred_knn)
        test_pred_knn = knn_best.predict(X_test_scaled)
        bal_acc_test_knn = accuracy_score(y_test, test_pred_knn)

        # Classificatore - RandomForest
        rf_best = RandomForestClassifier(
            n_estimators=500,         # Number of trees in the forest
            max_depth=50,             # Maximum depth of each tree
            min_samples_split=2,      # Minimum number of samples to split a node
            min_samples_leaf=1,       # Minimum number of samples per leaf node
            random_state=42           # For reproducibility
        )
        rf_best.fit(X_train_scaled, y_train)  # Train the model on scaled training data
        y_pred_train_rf = rf_best.predict(X_train_scaled)
        bal_acc_train_rf = accuracy_score(y_train, y_pred_train_rf)
        y_pred_val_rf = rf_best.predict(X_val_scaled)
        bal_acc_val_rf = accuracy_score(y_val, y_pred_val_rf)
        y_pred_test_rf = rf_best.predict(X_test_scaled)
        bal_acc_test_rf = accuracy_score(y_test, y_pred_test_rf)

        # Classificatore - Neural Network (MLP)
        nn_model = MLPClassifier(
            hidden_layer_sizes=(100,),  # one hidden layer with 100 neurons
            max_iter=1000,
            random_state=42
        )
        nn_model.fit(X_train_scaled, y_train)
        y_pred_train_nn = nn_model.predict(X_train_scaled)
        bal_acc_train_nn = accuracy_score(y_train, y_pred_train_nn)
        y_pred_val_nn = nn_model.predict(X_val_scaled)
        bal_acc_val_nn = accuracy_score(y_val, y_pred_val_nn)
        y_pred_test_nn = nn_model.predict(X_test_scaled)
        bal_acc_test_nn = accuracy_score(y_test, y_pred_test_nn)


        # Salva i risultati
        results.append({
            "n_euclidean": n_euclidean,
            "n_geometric": n_geometric,
            "SVM_train": train_bal_acc,
            "SVM_val": val_bal_acc,
            "SVM_test": test_bal_acc,
            #"AdaBoost_val": bal_acc_val,
            #"AdaBoost_test": bal_acc_test,
            "KNN_train": bal_acc_train_knn,
            "KNN_val": bal_acc_val_knn,
            "KNN_test": bal_acc_test_knn,
            "RF_train": bal_acc_train_rf,
            "RF_val": bal_acc_val_rf,
            "RF_test": bal_acc_test_rf,
            "MLP_train": bal_acc_train_nn,
            "MLP_val": bal_acc_val_nn,
            "MLP_test": bal_acc_test_nn
        })

# Risultati in DataFrame
df_results = pd.DataFrame(results)
print(df_results)

df_results['combination'] = df_results.apply(lambda row: f"({row['n_euclidean']},{row['n_geometric']})", axis=1)

def plot_balanced_accuracy(df, classifier_name):
    df_sorted = df.sort_values(by=['n_euclidean', 'n_geometric'])
    x = df_sorted['combination']
    y_val = df_sorted[classifier_name + '_val']
    y_train = df_sorted[classifier_name + '_train']

    plt.figure(figsize=(18, 6))
    plt.plot(x, y_val, label='Validation', marker='o')
    plt.plot(x, y_train, label='Train', marker='x')
    plt.xticks(rotation=90, fontsize=8)
    plt.xlabel('Combinazione (n_euclidean, n_geometric)')
    plt.ylabel('Balanced Accuracy')
    plt.title(f'Balanced Accuracy vs Feature Combinations - {classifier_name}')
    plt.legend()
    plt.tight_layout()
    plt.show()

classificatori = ['SVM', 'KNN', 'RF', 'MLP']


for clf in classificatori:
    plot_balanced_accuracy(df_results, clf)

idx_best = df_results['SVM_val'].idxmax()
best_val = df_results.loc[idx_best, 'SVM_val']
best_test = df_results.loc[idx_best, 'SVM_test']
best_comb = df_results.loc[idx_best, 'combination']